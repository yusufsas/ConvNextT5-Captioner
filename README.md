# ConvNextT5-Captioner
This repository presents an image captioning framework leveraging ConvNeXtV2 Base as the visual encoder and T5-base as the autoregressive text decoder. The architecture integrates a powerful convolutional backbone for high-quality visual feature extraction with a transformer-based language model for sequence generation.


# ConvNextT5-ImageCaptioning ðŸš€

A state-of-the-art image captioning pipeline combining ConvNeXtV2 Base encoder and T5-base decoder for generating descriptive natural language captions from images.
Model Architecture
The ConvnextT5CaptioningModel combines a powerful convolutional image encoder with a transformer-based text decoder for image captioning tasks.

Image Encoder:
Utilizes the ConvNeXtV2 Base model pretrained on ImageNet. The model extracts high-level visual features from input images. Global average pooling converts spatial features into a fixed-size feature vector of shape [batch_size, encoder_out_dim].

Projection Layer:
The extracted features are passed through a projection head â€” a series of fully connected layers with SiLU activation â€” to reduce the dimensionality from encoder_out_dim (e.g., 1024 or 2048) down to 768, matching the hidden size of the T5 decoder.

Text Decoder:
Uses Huggingfaceâ€™s T5ForConditionalGeneration pretrained model (t5-base). The projected visual features are reshaped and fed as a single-token encoder output to the T5 decoder, which generates captions autoregressively.

Forward Pass Workflow
Input images are processed by the ConvNeXtV2 encoder to extract feature vectors.

Features are projected to the T5 hidden dimension (768).

The projected features are unsqueezed to simulate a sequence length of 1 token for the decoder.

The T5 decoder receives these encoded features along with text input tokens (input_ids) and attention masks.

The model outputs caption tokens and computes the loss if target labels are provided.

Advantages
Leverages state-of-the-art visual and language models pretrained on large datasets.

End-to-end trainable architecture for image captioning.

Simple integration by projecting visual embeddings to match T5 input space.

Limitations and Future Work
Visual features are represented as a single token, which might limit capturing spatial information.

Adding multi-token visual embeddings or patch-wise features could improve contextual richness.

Experimentation with larger T5 variants or additional normalization layers may enhance performance.

## Table of Contents
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Model Architecture](#model-architecture)
- [Training](#training)
- [Evaluation](#evaluation)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)

## Features
- ConvNeXtV2 Base visual encoder for rich image feature extraction  
- T5-base transformer decoder for fluent caption generation  
- End-to-end training and inference pipeline  
- Support for standard captioning datasets

## Installation
```bash
git clone https://github.com/yusufsas/ConvNextT5-ImageCaptioning.git
cd ConvNextT5-ImageCaptioning
pip install -r requirements.txt
